## requests库

1.get 请求：

```python
import requests

data = {'word':'Hello!'}
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'}
response = requests.get('http://www.zhihu.com',params=data,headers=headers)
print(response.text)

```

2.post 请求：

```python
import requests

data = {'name':'gemey','age':'22'}
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'}
response = requests.post('http://httpbin.org/post',data = data,headers = headers)
print(response.json())
```

3.获取二进制内容：

```python
import requests
response = requests.get("https://github.com/favicon.ico")
with open('favicon.ico','wb') as f:
    f.write(response.content)
    f.close()
```

4.状态码错误判断：

```python
import requests

response = requests.get("http://hummer.vip")
exit() if not response.status_code == requests.codes.ok else print('Requests Successfully')

```

5.文件上传：

```python
import requests

files = {'file':open('favicon.ico','rb')}
response = requests.post("http://httpbin.org",files=files)
print(response.text)
exit() if not response.status_code == requests.ok else print('request successfuly')
```

6.保持会话--cookie

```python
import requests

s = requests.Session()
s.get('http://httpbin.org/cookies/set/number/123456789')
response = s.get('http://httpbin.org/cookies')
print(response.text)
```

7.代理设置

```python
import requests

proxies = {
    "http":"http://127.0.0.1:8080"
}
response = requests.get('https://www.hummer.vip')
print(response.status_code)
```

8.超时判断：

```python
import requests 
from requests.exceptions import ReadTimeout
try:
    response = requests.get('http://httpbin.org/get',timeout = 0.1)
    print(response.status_code)
except ReadTimeout:
    print('Timeout!')
```

9.登录设置：

```python
import requests 
from requests.auth import HTTPBasicAuth
response = requests.get('http://120.27.34.24:9001',auth=HttpBasicAuth('user','pw'))
print(response.status_code)
```



```python
import requests 
response = requests.get('http://120.27.34.24:9001',auth={'user':'pw'})
print(response.status_code)
```

## 正则表达式

```python
import re
content = 'Hello 1234567 Word_This is a Regex Demo'
result = re.match('^Hello\s(\d+)\sWord.*Demo$',content)
print(result)
print(result.group(1))
```

![image-20210130104206744](C:\Users\19026\AppData\Roaming\Typora\typora-user-images\image-20210130104206744.png)

贪婪匹配：

```python
import re
content = 'Hello 1234567 Word_This is a Regex Demo'
result = re.match('^He.*(\d+).*Demo$',content)
print(result)
print(result.group(1))
```

![image-20210130104744769](C:\Users\19026\AppData\Roaming\Typora\typora-user-images\image-20210130104744769.png)

非贪婪匹配：

```python
import re
content = 'Hello 1234567 Word_This is a Regex Demo'
result = re.match('^He.*?(\d+).*Demo$',content)
print(result)
print(result.group(1))
```

![image-20210130104818480](C:\Users\19026\AppData\Roaming\Typora\typora-user-images\image-20210130104818480.png)

匹配模式：‘.’一般来说不能匹配换行（\n)，但是加上匹配模式参数‘re.S’后就可以了

```python
import re
content = 'Hello 1234567 Word_This \n is a Regex Demo'
result = re.match('^He.*?(\d+).*Demo$',content,re.S)
print(result)
print(result.group(1))
```

![image-20210130105422888](C:\Users\19026\AppData\Roaming\Typora\typora-user-images\image-20210130105422888.png)

利用 re.search(' ',' ')可以从字符串中间开始匹配，而re.match()不可以

re.findall()可以匹配所有结果，而match和search方法只能匹配第一个结果

re.sub('zhengze','替换内容','原字符串')

![image-20210131205613833](C:\Users\19026\AppData\Roaming\Typora\typora-user-images\image-20210131205613833.png)

![image-20210131205726525](C:\Users\19026\AppData\Roaming\Typora\typora-user-images\image-20210131205726525.png)

re.compile(表达式，匹配模式)  编译一个正则对象

![image-20210131211814576](C:\Users\19026\AppData\Roaming\Typora\typora-user-images\image-20210131211814576.png)

## BeautifulSoup库

```python
from bs4 import BeautifulSoup
import requests

response = requests.get('https://www.9ku.com/geshou/2135.htm')
content = response.text
soup = BeautifulSoup(content,'lxml')
print(soup.title)
print(soup.head)
print(soup.p)
```

P标签只返回第一个

soup.title返回类型是tag型

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(content,'lxml')
print(soup.prettify) #对content进行格式化
print(soup.title.string)

soup.title    #标签选择器
soup.head
soup.p        #只返回第一个p标签
soup.p['name']  #获取属性
soup.p.string  #获取内容
soup.head.contents   #获取head的子节点
doup.p.parent    #获取父节点
```

findall()的用法：

```python
soup.find_all('name')
soup.find_all(attrs={'id':'list-1'}) #根据属性名查找
soup.find_all(id='list-1')
soup.find_all(class='element')
soup.find_all(text='Foo')   #返回结果是内容组成的列表
```

CSS选择器：

```python
soup.select('.panel')  #class值要加.
soup.select('#list-1') #id值要加#
soup.select('ul li')
```



<img src="C:\Users\19026\AppData\Roaming\Typora\typora-user-images\image-20210703155026765.png" alt="image-20210703155026765" style="zoom:200%;" />

![image-20210703155317489](C:\Users\19026\AppData\Roaming\Typora\typora-user-images\image-20210703155317489.png)



## PyQuery库

初始化：

```python 
from pyquery import PyQuery as pq
doc = pq(url='http://......')
doc = pq(filename='xxx.html')
```

CSS选择器

```python
doc('#containter .list ll')  #查找id=containter下的class=list下的li标签
doc.find('li')  #返回doc下的所有li标签
```

子，父标签：

```python
doc.childern('筛选条件')
doc.parents('筛选条件')
```

遍历：

```python
doc = pq(html)
lis = doc('li').item()  #产生器，遍历用
for li in lis:
    print(li)
```

获取属性：

```python
a = doc('.item-0 a')
print(a.attr('href'))
print(a.attr.href)
```

获取文本：

```python
print(a.text())
```

```python
print(a.html()) #获取html
```

DOM操作（修改节点）

```python
li = doc('.item.acctive')
li.removeClass('active')  #移除属性
li.addClass('active')		#添加属性
li.attr('name','link')  #修改属性，没有可以添加
li.css('font-size','14px') #修改css

wrap = doc('.wrap')
wrap.find('p').remove() #删除段落
```

